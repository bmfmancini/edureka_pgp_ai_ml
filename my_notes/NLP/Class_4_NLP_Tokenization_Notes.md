# Class 4 â€” NLP Tokenization & Text Processing (Catchâ€‘Up Notes)

## ğŸ“Œ Purpose of This File
These notes are designed to fully replace attendance for **Class 4**.
They include:
- A clear summary
- Detailed explanations
- Practical examples
- Learning resources (blogs & videos)

---

## ğŸ§  Highâ€‘Level Summary

This class focused on **tokenization**, the process of breaking raw text into smaller units (tokens) so machines can understand and process language.
Tokenization is the **foundation of all NLP pipelines**, including sentiment analysis, topic modeling, and large language models.

Key ideas:
- Why tokenization matters
- Types of tokenization
- Text normalization steps
- Common NLP pitfalls
- How tokenization impacts ML models

---

## ğŸ”¤ What Is Tokenization?

**Tokenization** is the process of splitting text into:
- Words
- Subâ€‘words
- Characters

Example:
```
"I love NLP!" â†’ ["I", "love", "NLP", "!"]
```

Why this matters:
- ML models cannot process raw text
- Tokens are mapped to numbers (vectors)
- Token quality affects model accuracy

---

## ğŸ§¹ Text Preâ€‘Processing Steps

### 1. Lowercasing
```
"Hello World" â†’ "hello world"
```

### 2. Removing punctuation
```
"Wow!!!" â†’ "wow"
```

### 3. Removing stopwords
Common words like:
- the, is, and, but

âš ï¸ Sometimes stopwords matter (e.g. **sentiment analysis**)

### 4. Normalization
- Removing extra spaces
- Handling accents
- Standardizing text

---

## âœ‚ï¸ Types of Tokenization

### 1. Word Tokenization
Splits by spaces and punctuation.

Pros:
- Simple
- Humanâ€‘readable

Cons:
- Struggles with slang & typos

---

### 2. Subword Tokenization
Breaks words into smaller meaningful pieces.

Example:
```
"unhappiness" â†’ ["un", "happy", "ness"]
```

Used by:
- BERT
- GPT models

Benefits:
- Handles unknown words
- Reduces vocabulary size

---

### 3. Character Tokenization
Each character becomes a token.

Pros:
- Languageâ€‘agnostic

Cons:
- Very long sequences
- Less semantic meaning

---

## âš ï¸ Common Tokenization Challenges

- Emojis ğŸ˜ƒ
- Hashtags (#NLP)
- URLs
- Contractions ("don't")
- Domainâ€‘specific language

Poor tokenization = poor model performance

---

## ğŸ§ª Why Tokenization Impacts Models

Tokenization affects:
- Vocabulary size
- Model memory usage
- Training time
- Accuracy

Example:
Bad tokens â†’ noisy features â†’ weak predictions

---

## ğŸ§° Common NLP Libraries

### Python Libraries
- **NLTK**
- **spaCy**
- **scikitâ€‘learn**
- **HuggingFace Tokenizers**

---

## ğŸ§ª Example (Python)

```python
from sklearn.feature_extraction.text import CountVectorizer

texts = ["I love NLP", "NLP loves data"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

print(vectorizer.get_feature_names_out())
```

---

## ğŸ“š Recommended Reading (Blogs)

- https://towardsdatascience.com/tokenization-in-nlp
- https://machinelearningmastery.com/natural-language-processing/
- https://huggingface.co/docs/tokenizers

---

## ğŸ¥ Recommended YouTube Videos

- "Tokenization Explained Simply" â€“ StatQuest
- "NLP Preprocessing Tutorial" â€“ freeCodeCamp
- "How BERT Tokenization Works" â€“ HuggingFace

---

## âœ… What You Should Know for Exams / Assignments

You should be able to:
- Define tokenization
- Explain different token types
- Describe preprocessing steps
- Explain why tokenization matters
- Identify tokenization challenges

---

## ğŸ“ Key Takeaway

Tokenization is **not just a technical step** â€” it fundamentally shapes how machines understand language.
Good tokenization leads to better features, better models, and better results.

---

*Prepared as a complete catchâ€‘up reference.*
