{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2ag4oDNf0L-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LInujJFe_wZE",
        "outputId": "92e5ad6f-9f80-4913-fc45-297c1fc5fa73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.4.59)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers torch sentencepiece accelerate bitsandbytes langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOlIPSzf7RU"
      },
      "source": [
        "TEXT Summarization Excerisze with Falconsai model - Smallts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "a9019b73e13849cdbfb61c22f9d07eb9",
            "e21a3e6507f0442ba2b9cd9f0cedacde",
            "9887d7331ac44473ba21ca527c65bb8f",
            "f742d5e61fa348a98d96a3f960161619",
            "e392bc8d48664fd3b47c654143ebeb08",
            "c25251ff775042b39ddf60988da9c92a",
            "be94433d2a7441298a19cd73ac132c31",
            "614b4983a6944b0fa13f69195189943f",
            "883c97579e8c403c9c7ea18d69c7c802",
            "9b30a4ae38a641e5960a3898b09a230b",
            "0ce7f874920546dab73170358f43f5af",
            "464fbb814f35437dbe717d4c0f041726",
            "5dc04f268feb4238aaa5c4600f9d9e1b",
            "7c67878e694e41c8b3f5187b7149c458",
            "9425a32f79ad49c8a07d73efa042ba63",
            "36b8e3fe091a4063b7c02defb3b83c12",
            "e980ab96423a4675bb3d2090f468feb8",
            "8d67b374ad0f4d5da2d1e86c034aa7a3",
            "66e9ba9bccf4495a88a3ec92aa05329c",
            "10dd7b93a10a4fd786577fa0f1c73d33",
            "75bd8c6b69864226afcb696ddb9545ee",
            "9bac85773da84bf5b2d19fbe089a9d3d",
            "189b9ce1fdfe43679909539db2ef3258",
            "38adf0471e804f67be96e4e90ef68f0d",
            "d2635ed612e54d83aaca73ba879407c3",
            "474e51661c8047f1bb4b99fe8dd2a444",
            "f32b362bd40d4810b8e03b583c9f8da5",
            "671b0ff67d394ead935ab32d7d3189ff",
            "fa22e1db83604013afc89bf396b3e8ca",
            "1278f70ef2834ec3b4ccf349c67e4ea6",
            "c778b7ed691a4dc79dcb1c01e58a7581",
            "bbde7abae4c340fda0790e402fca10f8",
            "0045e5f164d3415aafd6221ba192cee8",
            "5485f53c7ad14e8ea728fe3cf1a86d72",
            "982e9f95815a4ca994e7e1bc58f804c2",
            "c892249ea86844ec80680df50af8fd44",
            "087a262def6149cd9fb25137302387e1",
            "8a5fa55ef22f4dfd88cb02457bf4fcb2",
            "be63b7133b9249a6b3977ae25ad86bdf",
            "55120e56b2074daaa66c6e4051c842c5",
            "8b0175dd3fde434ea44782572055ab2c",
            "bcc9600579cf4ab2a7b066118be51948",
            "0ad08909ec8f4f6391cf189ce286d19e",
            "87950b8837cf43a2abda1ca69f0242e8",
            "6ef599f520fe423f9b01fa04972dcfc8",
            "afe79b7013c347bda2da75594c1cb72e",
            "488176e6340a45458b9b874cd7f0f683",
            "3b98010b88ed4239845ecbc83fe2a794",
            "b24643129e7241559ba613dc2cb172f6",
            "b7f1c27a072d4a17b4601b61a125201f",
            "bcbdb9f6363a440581e9fb50f9ca96b2",
            "eedc87a806d34eef8b18f57d690facdd",
            "723aa03fdf13480e9f4c69fb5fd0aa6d",
            "68c45e5656c64c619d6a48912a27dbdc",
            "141c8869485643babe336c19fc13d840",
            "ef2b182eb6de4f8ab4faefc0babe9d2d",
            "9c50f673b2c842d1bba23bc783e0e261",
            "fa39d7cd899e484c9346fe753b87ef23",
            "98375cbc5ea742879ed3ad2a90c6cfac",
            "1f508a89b69d4a258426cb0af6a20059",
            "e0a3bd3c3eda47a9b863c79129b7dc0a",
            "f44f929baa6043ca89fb8fe92fface66",
            "0b2524871853432da7e15d4df57facd5",
            "3656431e7fd642d3a96c5f1c212bb355",
            "97d95c9bd2dc40148138d32aaf52c160",
            "043c7bd4ca5c4be5a9e6757bb5d475b4",
            "5bab5d7034974d1eba99bd0e253186db",
            "ad3ce0f4663f4fff9e636a49d9f76f39",
            "9995a7aa4fed458ea2cfa1a5672ae1d5",
            "034a86b0ac3346dfae369a0aa9c35694",
            "f83078e8e75240ed8e47753e6634c471",
            "9312a0198d674fe093525f19f1cf6c4f",
            "f0b3c586f5694a3ba8b80b524983db8d",
            "8c464151b350414bb2c4c8e216468c60",
            "6451978489c54bc2ad7902a2729b0790",
            "5cd42df1ffca4570bc14f80e0fe1a13c",
            "b5ed8e61ae464a018ed7a6bfe905ebe1"
          ]
        },
        "id": "ZjswsmmRf-nv",
        "outputId": "893023d0-a18a-43c5-d71a-37377796a7d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9019b73e13849cdbfb61c22f9d07eb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "464fbb814f35437dbe717d4c0f041726",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "189b9ce1fdfe43679909539db2ef3258",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5485f53c7ad14e8ea728fe3cf1a86d72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ef599f520fe423f9b01fa04972dcfc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef2b182eb6de4f8ab4faefc0babe9d2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bab5d7034974d1eba99bd0e253186db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "summarize = pipeline(\"summarization\",\n",
        "                     model=\"Falconsai/text_summarization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "tR1vGuRcV14h",
        "outputId": "bf10df1d-b752-450a-de51-7309861f93b4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5628ceb-6383-447a-847e-3cf44f1e7866\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5628ceb-6383-447a-847e-3cf44f1e7866\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving test_sentences.txt to test_sentences.txt\n",
            "User uploaded file \"test_sentences.txt\" with length 12210 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "document_content = []\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  content = uploaded[fn].decode('utf-8')\n",
        "  document_content.append(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "96661ab4"
      },
      "outputs": [],
      "source": [
        "def print_summaries_to_console(text_chunks, all_summaries):\n",
        "    \"\"\"\n",
        "    Prints the original text's chunking information, summaries for each chunk,\n",
        "    and a consolidated summary to the console.\n",
        "\n",
        "    Args:\n",
        "        text_chunks (list): A list of strings, where each string is a chunk of the original text.\n",
        "        all_summaries (list): A list of strings, where each string is the summary for a corresponding text chunk.\n",
        "    \"\"\"\n",
        "    print(f\"Original text split into {len(text_chunks)} chunks for summarization.\\n\")\n",
        "\n",
        "    print(\"--- Summaries per Chunk ---\")\n",
        "    for i, chunk_summary_text in enumerate(all_summaries):\n",
        "        print(f\"\\nChunk {i+1}/{len(all_summaries)}:\")\n",
        "        print(chunk_summary_text)\n",
        "\n",
        "    print(\"\\n--- Consolidated Summary (joining all chunk summaries) ---\")\n",
        "    final_summary = \" \".join(all_summaries)\n",
        "    print(final_summary.replace('. ', '.\\n'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e417c245",
        "outputId": "aad92f52-c5e7-4b34-db70-421ec782b05f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2461 > 512). Running this sequence through the model will result in indexing errors\n",
            "Your max_length is set to 200, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text split into 7 chunks for summarization.\n",
            "\n",
            "--- Summaries per Chunk ---\n",
            "\n",
            "Chunk 1/7:\n",
            "On November 18, 2025, Cloudflare\u2019s network experienced significant failures to deliver network traffic for approximately two hours and ten minutes . Nearly three weeks later, our network again failed to serve traffic for 28% of applications behind our network for about 25 minutes. We published detailed post-mortem blog posts following both incidents, but we know that we have more to do to earn back your trust. Today we are sharing details about the work underway at Cloudfleare to prevent outages like these from happening again.\n",
            "\n",
            "Chunk 2/7:\n",
            "We expect Cloudflare\u2019s network to be much more resilient . Break glass procedures allow certain individuals to elevate privilege under certain circumstances to perform urgent actions to resolve high severity scenarios . In the moments leading up to each incident we instantaneously deployed a configuration change in our data centers in hundreds of cities around the world . The November change was an automatic update to our Bot Management classifier .\n",
            "\n",
            "Chunk 3/7:\n",
            "We deploy first to employee traffic, before carefully rolling out the change to increasing percentages of customers worldwide, starting with free users . If we detect an anomaly at any stage, we can revert the release without any human intervention . The past two incidents have demonstrated that we need to treat any change that is applied to how we serve traffic in our network with the same level of tested caution that we apply to changes to the software itself .\n",
            "\n",
            "Chunk 4/7:\n",
            "Work is underway to treat configuration the same way that we treat code . Each team at Cloudflare that owns a service must define metrics that indicate a deployment has succeeded or failed, the rollout plan, and the steps to take if it does not succeed . Different services will have slightly different variables . Some might need longer wait times before proceeding to more data centers . Others might have lower tolerances for error rates even if they cause false positive signals .\n",
            "\n",
            "Chunk 5/7:\n",
            "to go back to our bot Management service failure, there were at least two key interfaces where, if we assumed failure was going to happen, we could have handled it gracefully . the first was in the interface that read the corrupted config file . Instead of panicking, there should have been a sane set of validated defaults which would have allowed traffic to pass through our network . In the event that our bot management module failed, we should not have dropped traffic by default .\n",
            "\n",
            "Chunk 6/7:\n",
            "Our team will be reviewing and improving all of the break glass procedures and technology to ensure that, when necessary, we can access the right tools . This includes reviewing and removing circular dependencies, or being able to \u201cbypass\u201d them quickly in the event there is an incident . We will also increase the frequency of our training exercises, so that processes are well understood by all teams prior to any potential disaster scenario in the future . By the end of Q1, and largely before then, we will: Ensure all production systems are covered\n",
            "\n",
            "Chunk 7/7:\n",
            "We failed our users and the Internet as a whole in these past two incidents . We plan to share updates as this work proceeds .\n",
            "\n",
            "--- Consolidated Summary (joining all chunk summaries) ---\n",
            "On November 18, 2025, Cloudflare\u2019s network experienced significant failures to deliver network traffic for approximately two hours and ten minutes .\n",
            "Nearly three weeks later, our network again failed to serve traffic for 28% of applications behind our network for about 25 minutes.\n",
            "We published detailed post-mortem blog posts following both incidents, but we know that we have more to do to earn back your trust.\n",
            "Today we are sharing details about the work underway at Cloudfleare to prevent outages like these from happening again.\n",
            "We expect Cloudflare\u2019s network to be much more resilient .\n",
            "Break glass procedures allow certain individuals to elevate privilege under certain circumstances to perform urgent actions to resolve high severity scenarios .\n",
            "In the moments leading up to each incident we instantaneously deployed a configuration change in our data centers in hundreds of cities around the world .\n",
            "The November change was an automatic update to our Bot Management classifier .\n",
            "We deploy first to employee traffic, before carefully rolling out the change to increasing percentages of customers worldwide, starting with free users .\n",
            "If we detect an anomaly at any stage, we can revert the release without any human intervention .\n",
            "The past two incidents have demonstrated that we need to treat any change that is applied to how we serve traffic in our network with the same level of tested caution that we apply to changes to the software itself .\n",
            "Work is underway to treat configuration the same way that we treat code .\n",
            "Each team at Cloudflare that owns a service must define metrics that indicate a deployment has succeeded or failed, the rollout plan, and the steps to take if it does not succeed .\n",
            "Different services will have slightly different variables .\n",
            "Some might need longer wait times before proceeding to more data centers .\n",
            "Others might have lower tolerances for error rates even if they cause false positive signals .\n",
            "to go back to our bot Management service failure, there were at least two key interfaces where, if we assumed failure was going to happen, we could have handled it gracefully .\n",
            "the first was in the interface that read the corrupted config file .\n",
            "Instead of panicking, there should have been a sane set of validated defaults which would have allowed traffic to pass through our network .\n",
            "In the event that our bot management module failed, we should not have dropped traffic by default .\n",
            "Our team will be reviewing and improving all of the break glass procedures and technology to ensure that, when necessary, we can access the right tools .\n",
            "This includes reviewing and removing circular dependencies, or being able to \u201cbypass\u201d them quickly in the event there is an incident .\n",
            "We will also increase the frequency of our training exercises, so that processes are well understood by all teams prior to any potential disaster scenario in the future .\n",
            "By the end of Q1, and largely before then, we will: Ensure all production systems are covered We failed our users and the Internet as a whole in these past two incidents .\n",
            "We plan to share updates as this work proceeds .\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "'''\n",
        "Initialize the tokenizer for the model to accurately count tokens.\n",
        "Since the model has both an encoder and decoder, the model supports no more than 512 tokens.\n",
        "We need to split the input for larger texts.\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
        "\n",
        "def split_text_by_tokens(text, tokenizer, max_chunk_tokens=None, overlap_tokens=50):\n",
        "    \"\"\"\n",
        "    Splits a long text into chunks of token_ids, then decodes them back to text.\n",
        "    This is crucial for models with a maximum sequence length, like the summarization model.\n",
        "    \"\"\"\n",
        "    if max_chunk_tokens is None:\n",
        "        '''\n",
        "        Use the model's max length and provide a buffer\n",
        "        '''\n",
        "        max_chunk_tokens = tokenizer.model_max_length - 50\n",
        "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    chunks = []\n",
        "    current_start_idx = 0\n",
        "\n",
        "    while current_start_idx < len(token_ids):\n",
        "        current_end_idx = min(current_start_idx + max_chunk_tokens, len(token_ids))\n",
        "\n",
        "        '''\n",
        "        Decode the tokens back to text for summarization\n",
        "        '''\n",
        "        chunk_text = tokenizer.decode(token_ids[current_start_idx:current_end_idx], skip_special_tokens=True)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "        '''\n",
        "        Move the start index for the next chunk, accounting for overlap\n",
        "        '''\n",
        "        next_start_idx = current_start_idx + (max_chunk_tokens - overlap_tokens)\n",
        "\n",
        "        '''\n",
        "        If the next_start_idx would go past the end, break the loop.\n",
        "        The remaining part will be handled by the logic after the loop.\n",
        "        '''\n",
        "        if next_start_idx >= len(token_ids):\n",
        "            break\n",
        "\n",
        "        current_start_idx = next_start_idx\n",
        "\n",
        "    '''\n",
        "    Handle any remaining tokens that might not have been captured as a full chunk due to overlap calculation.\n",
        "    This ensures the very end of the text is always included, and avoids duplicate chunks if the last one was already covered.\n",
        "    '''\n",
        "    if current_start_idx < len(token_ids):\n",
        "        remaining_text = tokenizer.decode(token_ids[current_start_idx:], skip_special_tokens=True).strip()\n",
        "        if remaining_text and (not chunks or remaining_text != chunks[-1].strip()):\n",
        "            chunks.append(remaining_text)\n",
        "\n",
        "    '''\n",
        "    Remove empty chunks that might arise from edge cases\n",
        "    '''\n",
        "    chunks = [chunk for chunk in chunks if chunk.strip()]\n",
        "\n",
        "    return chunks\n",
        "\n",
        "'''\n",
        "Check if 'document_content' is defined, if not, try to populate it from 'uploaded'\n",
        "'''\n",
        "if 'document_content' not in locals() and 'document_content' not in globals():\n",
        "    if 'uploaded' in locals() and uploaded:\n",
        "        '''\n",
        "        Assuming only one file is uploaded for this context\n",
        "        '''\n",
        "        first_fn = list(uploaded.keys())[0]\n",
        "        document_content = [uploaded[first_fn].decode('utf-8')]\n",
        "    else:\n",
        "        raise NameError(\"document_content is not defined and no uploaded file found to populate it. Please run the file upload cell first.\")\n",
        "\n",
        "'''\n",
        "Assuming 'document_content' contains the entire document as a single string at index 0.\n",
        "This was established in a previous step.\n",
        "'''\n",
        "full_text = document_content[0]\n",
        "\n",
        "'''\n",
        "Split the full text into chunks that fit the model's token limit (e.g., 450 tokens).\n",
        "This is done to avoid the 'Token indices sequence length is longer' error.\n",
        "'''\n",
        "text_chunks = split_text_by_tokens(full_text, tokenizer, max_chunk_tokens=450, overlap_tokens=50)\n",
        "\n",
        "all_summaries = []\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    '''\n",
        "    The pipeline expects a list of strings, even for a single input\n",
        "    '''\n",
        "    chunk_summary_result = summarize([chunk])\n",
        "    summary_text = chunk_summary_result[0]['summary_text']\n",
        "    all_summaries.append(summary_text)\n",
        "\n",
        "'''\n",
        "Call the new function to print the summaries\n",
        "'''\n",
        "print_summaries_to_console(text_chunks, all_summaries)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}