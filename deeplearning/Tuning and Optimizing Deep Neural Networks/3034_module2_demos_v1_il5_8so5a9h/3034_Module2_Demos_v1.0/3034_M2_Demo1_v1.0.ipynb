{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1 - CNN-based image classification using CIFAR-10\n",
        "\n",
        "##**Scenario:**\n",
        "You are building an image classification system for a company that wants to automatically tag images into categories like airplane, car, bird, cat, etc. The dataset provided is CIFAR-10 (10 classes, small RGB images).\n",
        "\n",
        "##**Objective:**\n",
        "To train a CNN model on CIFAR-10 while applying model optimization techniques like learning rate scheduling, normalization, regularization, data augmentation, and visualize training using TensorBoard.\n",
        "\n"
      ],
      "metadata": {
        "id": "EIlR_zznQt63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Required Libraries\n",
        " Import all required libraries for model building, training, and visualization.\n"
      ],
      "metadata": {
        "id": "0r7kPrphRP1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "RKwvvn1yQsg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Load and Preprocess the CIFAR-10 Dataset\n",
        "Load CIFAR-10, normalize pixel values, and one-hot encode labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "VSw0SIMHRSXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "QvKz-BePRRir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Set Up Data Augmentation\n",
        "Use ImageDataGenerator to augment training data to improve generalization."
      ],
      "metadata": {
        "id": "VMjuettRRtPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "CUCmT8ZoRygf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define the CNN Model with Regularization and Batch Normalization\n",
        "Build a CNN with dropout, L2 regularization, and batch normalization to combat overfitting and improve training."
      ],
      "metadata": {
        "id": "PZ8cSXleRyQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
        "                            kernel_regularizer=regularizers.l2(0.001), input_shape=(32, 32, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()"
      ],
      "metadata": {
        "id": "NpYmIp_CR_MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Compile the Model with Optimizer and Learning Rate Scheduling\n",
        "Compile the model with Adam optimizer, gradient clipping, and add learning rate scheduler to reduce LR on plateau.\n"
      ],
      "metadata": {
        "id": "3uJBKUvOSNL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)"
      ],
      "metadata": {
        "id": "yM-rxcRUSSo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Setup TensorBoard and ModelCheckpoint\n",
        "Enable TensorBoard for visualizing training progress and add checkpoint to save best model."
      ],
      "metadata": {
        "id": "utUyNIAdSgcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/demo1\"\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "AY2vTgaqSdB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train the Model with Augmentation\n",
        "Train the model using augmented data with TensorBoard, checkpointing, and LR scheduling."
      ],
      "metadata": {
        "id": "O99RFrM4SqN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    epochs=2,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[tensorboard_callback, checkpoint, lr_scheduler])"
      ],
      "metadata": {
        "id": "ALVgRarvSoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Launch TensorBoard (from terminal)\n",
        "Running this in terminal to start TensorBoard and view training curves, gradients, histograms, and model graph."
      ],
      "metadata": {
        "id": "BqaYu33-Sy1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/demo1"
      ],
      "metadata": {
        "id": "Plo4gtvDSxct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "km8qESgkSrxr"
      }
    }
  ]
}