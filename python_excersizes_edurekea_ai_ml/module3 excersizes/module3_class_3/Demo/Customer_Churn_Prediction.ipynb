{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Demo- Customer Churn Prediction**\n",
        "#**Scenario**\n",
        "\n",
        "A telecom company wants to predict whether a customer will churn (leave the service) or stay based on their usage patterns, complaints, contract type, and other factors.\n",
        "\n",
        "##**Objectives**\n",
        "Build a classification model to predict customer churn (Binary Classification: Churn = Yes/No).\n",
        "\n",
        "* Compare different classification algorithms (Logistic Regression, Decision Tree, Random Forest, SVM, Neural Networks, etc.).\n",
        "\n",
        "* Evaluate model performance using accuracy, precision, recall, F1-score, and AUC-ROC.\n",
        "\n",
        "* Add a Predicted_Churn column using the best-performing model and visualize actual vs. predicted churn counts with a bar chart.\n"
      ],
      "metadata": {
        "id": "9SrQZ5HWdJ_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Importing Necessary Libraries and Data Loading\n",
        "\n"
      ],
      "metadata": {
        "id": "0DvFJGwBfyAn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd-sM7Q0Vss1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder # Import LabelEncoder here\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('customer_churn_dataset.csv')"
      ],
      "metadata": {
        "id": "EwoPn-_KFcYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Preprocessing\n",
        "Convert categorical variables to numerical format, as machine learning models work with numbers.\n"
      ],
      "metadata": {
        "id": "3HLBucdfmyR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical columns like 'Complaints', 'TechSupport', and 'ContractType' to numeric using LabelEncoder\n",
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "z4A6EHy5XxkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply label encoding to 'Complaints' and 'TechSupport' (binary columns)\n",
        "df['Complaints'] = label_encoder.fit_transform(df['Complaints'])\n",
        "df['TechSupport'] = label_encoder.fit_transform(df['TechSupport'])"
      ],
      "metadata": {
        "id": "pwmSHYV2FZvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For 'ContractType', we'll use one-hot encoding (since it has more than two categories)\n",
        "df = pd.get_dummies(df, columns=['ContractType'], drop_first=True)"
      ],
      "metadata": {
        "id": "-wRTMpplFUH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Feature Scaling & Train-Test Split\n",
        "\n",
        "To ensure fair learning, we standardize numerical features and split the data into training and testing sets—this helps prevent overfitting."
      ],
      "metadata": {
        "id": "5Di1x6edf62K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features and target\n",
        "X = df.drop(columns=['CustomerID', 'Churn'])\n",
        "y = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Convert 'Yes'/'No' to 1/0"
      ],
      "metadata": {
        "id": "obNALIllD2qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "cOr76XyTFRnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "0wJ86cFzFPe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Model Implementation\n",
        "\n",
        "Time to put machine learning to work! We train five models—Logistic Regression, Decision Tree, Random Forest, SVM, and KNN—to predict customer churn."
      ],
      "metadata": {
        "id": "1fMz3oldgAiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(f\"Classification Report for {model.__class__.__name__}:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC: {auc:.4f}\\n\")"
      ],
      "metadata": {
        "id": "iAgHxtxjDyLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Accuracy: Measures the percentage of correct predictions, but may not be reliable in imbalanced datasets.\n",
        "\n",
        "* Precision: Important when false positives need to be minimized (e.g., avoiding false churn alerts).\n",
        "\n",
        "* Recall: Crucial when false negatives are costly (e.g., missing actual churners).\n",
        "\n",
        "* F1-Score: A balance between precision and recall, useful for overall performance.\n",
        "\n",
        "* ROC-AUC Score: Measures the model’s ability to distinguish between classes, important for churn detection."
      ],
      "metadata": {
        "id": "yRiV_lWZp3JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "print(\"Logistic Regression Results:\")\n",
        "evaluate_model(log_reg, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "fTVOuu1hXtZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "print(\"Decision Tree Results:\")\n",
        "evaluate_model(dt_model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "fUCJaw8YDvfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "print(\"Random Forest Results:\")\n",
        "evaluate_model(rf_model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "mm2imD91DsKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Nearest Neighbors (KNN)\n",
        "knn_model = KNeighborsClassifier()\n",
        "print(\"K-Nearest Neighbors Results:\")\n",
        "evaluate_model(knn_model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "UNKsC9MpDqus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "print(\"Support Vector Machine Results:\")\n",
        "evaluate_model(svm_model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "uJssWsF2Do87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Hyperparameter Tuning\n",
        "Because default settings aren’t always the best, we fine-tune our models using Grid Search and Randomized Search to squeeze out maximum performance."
      ],
      "metadata": {
        "id": "ANBLL4WHgHri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with Randomized Search (Logistic Regression)\n",
        "param_dist_log_reg = {\n",
        "    'C': np.logspace(-3, 3, 7),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "random_search_log_reg = RandomizedSearchCV(log_reg, param_dist_log_reg, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "random_search_log_reg.fit(X_train, y_train)\n",
        "print(\"Best Parameters from Random Search for Logistic Regression:\")\n",
        "print(random_search_log_reg.best_params_)"
      ],
      "metadata": {
        "id": "90XITsVYDkOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with Grid Search (Decision Tree)\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 10, 20, None],  # Depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples to split\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples at leaf node\n",
        "    'criterion': ['gini', 'entropy']  # Splitting criterion\n",
        "}\n",
        "grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "print(\"Best Parameters from Grid Search for Decision Tree:\")\n",
        "print(grid_search_dt.best_params_)"
      ],
      "metadata": {
        "id": "9lTwZxrhfAeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with Grid Search (Random Forest)\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "print(\"Best Parameters from Grid Search for Random Forest:\")\n",
        "print(grid_search_rf.best_params_)"
      ],
      "metadata": {
        "id": "P2k-FJLQXq_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with Grid Search (SVM)\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
        "    'gamma': ['scale', 'auto']  # Kernel coefficient for ‘rbf’ and ‘poly’\n",
        "}\n",
        "\n",
        "grid_search_svm = GridSearchCV(SVC(probability=True, random_state=42), param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "print(\"Best Parameters from Grid Search for SVM:\")\n",
        "print(grid_search_svm.best_params_)"
      ],
      "metadata": {
        "id": "PsBuKVOuezwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning with Grid Search (KNN)\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "print(\"Best Parameters from Grid Search for KNN:\")\n",
        "print(grid_search_knn.best_params_)"
      ],
      "metadata": {
        "id": "MMzFxAzhDi3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Best Model Selection\n",
        "\n",
        "Finally, we compare all the models, rank them based on their scores, and crown the best one for predicting customer churn—because only the best should go into production!Evaluating each bestfit model"
      ],
      "metadata": {
        "id": "Vkf4Kr8YgrES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best Logistic Regression model from Random Search\n",
        "best_log_reg = random_search_log_reg.best_estimator_\n",
        "print(\"Best Logistic Regression Results (after Random Search):\")\n",
        "evaluate_model(best_log_reg, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "TEmxUhaOal8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best Decision Tree model from Grid Search\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "print(\"Best Decision Tree Results (after Grid Search):\")\n",
        "evaluate_model(best_dt, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "nSU6mq1le-iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best Random Forest model from Grid Search\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "print(\"Best Random Forest Results (after Grid Search):\")\n",
        "evaluate_model(best_rf, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "f_EYOLfaajzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best SVM model from Grid Search\n",
        "best_svm = grid_search_svm.best_estimator_\n",
        "print(\"Best SVM Results (after Grid Search):\")\n",
        "evaluate_model(best_svm, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "4T02cWIrem7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best KNN model from Grid Search\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "print(\"Best KNN Results (after Grid Search):\")\n",
        "evaluate_model(best_knn, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "HcIEaZZUFJDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of models to compare\n",
        "models = {\n",
        "    'Logistic Regression': best_log_reg,\n",
        "    'Decision Tree': dt_model,\n",
        "    'Random Forest': best_rf,\n",
        "    'KNN': best_knn,\n",
        "    'SVM': svm_model  # or best_svm if you tune it later\n",
        "}\n",
        "\n",
        "# Compare metrics\n",
        "comparison_results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    result = {\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1 Score': f1_score(y_test, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else 'N/A'\n",
        "    }\n",
        "    comparison_results.append(result)\n",
        "\n",
        "# Convert to DataFrame\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(\"Model Comparison Summary:\")\n",
        "display(comparison_df.sort_values(by='F1 Score', ascending=False))\n"
      ],
      "metadata": {
        "id": "y_K6S6RahYnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Performance Evaluation\n",
        "\n",
        "Let’s see how well our models perform! We check confusion matrices, and ROC-AUC to find the strongest contender."
      ],
      "metadata": {
        "id": "T6WN4Lffl7ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "def plot_conf_matrix_and_roc(model, X_test, y_test, model_name):\n",
        "    # Confusion Matrix\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.title(f'{model_name} - ROC Curve')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "nOmJWDo3aggv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_conf_matrix_and_roc(best_log_reg, X_test, y_test, \"Best Logistic Regression\")\n",
        "plot_conf_matrix_and_roc(best_dt, X_test, y_test, \"Best Decision Tree\")\n",
        "plot_conf_matrix_and_roc(best_rf, X_test, y_test, \"Best Random Forest\")\n",
        "plot_conf_matrix_and_roc(best_svm, X_test, y_test, \"Best SVM\")\n",
        "plot_conf_matrix_and_roc(best_knn, X_test, y_test, \"Best KNN\")"
      ],
      "metadata": {
        "id": "G_nO-3oua1-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare all models and select the best\n",
        "model_scores = {\n",
        "    \"Logistic Regression\": accuracy_score(y_test, best_log_reg.predict(X_test)),\n",
        "    \"Decision Tree\": accuracy_score(y_test, best_dt.predict(X_test)),\n",
        "    \"Random Forest\": accuracy_score(y_test, best_rf.predict(X_test)),\n",
        "    \"KNN\": accuracy_score(y_test, best_knn.predict(X_test)),\n",
        "    \"SVM\": accuracy_score(y_test, best_svm.predict(X_test))\n",
        "}\n",
        "\n",
        "best_model_name = max(model_scores, key=model_scores.get)\n",
        "print(f\"The best model is: {best_model_name} with Accuracy: {model_scores[best_model_name]:.4f}\")"
      ],
      "metadata": {
        "id": "uOubq69Vqf1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Predicted Churn Column and Comparison Graph\n",
        "Add a column Predicted_Churn to the dataset using the best-performing model and plot a bar chart comparing actual vs. predicted churn counts."
      ],
      "metadata": {
        "id": "79RexNZ3uoPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best-performing model (replace 'best_model' with your actual best model)\n",
        "best_model = best_rf  # Assuming Random Forest performed best; replace if needed\n",
        "\n",
        "# Make predictions on the entire dataset\n",
        "df['Predicted_Churn'] = best_model.predict(X)\n",
        "\n",
        "# Convert numeric predictions back to 'Yes'/'No' for readability\n",
        "df['Predicted_Churn'] = df['Predicted_Churn'].apply(lambda x: 'Yes' if x == 1 else 'No')\n",
        "\n",
        "# Save the updated dataset with predictions\n",
        "df.to_csv('customer_churn_predictions.csv', index=False)\n",
        "print(\"Updated dataset with predicted churn saved as 'customer_churn_predictions.csv'.\")"
      ],
      "metadata": {
        "id": "Bh2w-5puu4H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count actual and predicted churn values\n",
        "actual_counts = df['Churn'].value_counts()\n",
        "predicted_counts = df['Predicted_Churn'].value_counts()\n",
        "\n",
        "# Combine into a DataFrame\n",
        "churn_comparison = pd.DataFrame({'Actual': actual_counts, 'Predicted': predicted_counts}).T\n",
        "\n",
        "# Stacked Bar Chart\n",
        "churn_comparison.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='viridis')\n",
        "\n",
        "plt.title('Comparison of Actual vs Predicted Churn')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Churn Status', labels=['No Churn', 'Churn'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wKHgSSm2xl8h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}